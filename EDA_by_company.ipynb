{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is this?\n",
    "## This is a notebook which takes data scraped from trust pilot, and explores the data\n",
    "\n",
    "#### Summay\n",
    "- We cover basic data transformation such as tokenization and lemmatization, then general EDA on the structure of the reviews, such as words per review, broken down by 1-5 stars etc..., then we move to deeper analysis, such as bigrams, then more advanced modelling with NMF and LDA with the aim of distilling all of our reviews into topics. Next we built word clouds, and investigate reviews over time, and different trends, such as by weekday. After this we build a logistic regression model, and use ELI5 to find get insights as to how words correspond to reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to run this notebook - from scratch\n",
    "### 1 - Get the data\n",
    "- Run the run_new_website script, with the desired company url\n",
    "- Do some initial data quality checks - e.g. number of unique reviews match (roughly) the website etc...\n",
    "\n",
    "### 2 - Run the first 2 steps in this notebook, to clean the data\n",
    "- First import the packages, then read in the data from the correct csv, then clean the data using `clean_data`\n",
    "\n",
    "### 3 - Remove stopwords\n",
    "- First run the `find_custom_stopwords` function, and use that list to define custom words to remove, for example, the name of the company\n",
    "- And replace original reviews, with the clean versions (this is optional, but advised)\n",
    "\n",
    "### 4 - Run the rest of the notebook to get the desired visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# libraries for visualization\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "\n",
    "# For latent dirichlet allocation\n",
    "import spacy\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# For modelling and ELI5 analysis\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "import eli5\n",
    "\n",
    "## Custom\n",
    "# Used for plotting visualisations\n",
    "from plotting_trust import *\n",
    "\n",
    "# Used for cleaning (lemmatizing, removing punctuation, lower case etc...)\n",
    "from clean_words import *\n",
    "\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: read in data\n",
    "data = pd.read_csv('data/20200705_morrisons_com_trustpilot.csv')\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: clean data\n",
    "data = clean_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3a: How do we know which words to remove\n",
    "a = [item for sublist in data['content_clean'].str.split().values for item in sublist]\n",
    "pd.Series(a).value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3b: remove custom stopwords\n",
    "custom_stopwords = ['dfs']\n",
    "data = remove_custom_stopwords(df = data,\n",
    "                        custom_stop = custom_stopwords,\n",
    "                        cols = ['title_clean', 'content_clean'])\n",
    "# Step 4: replace content and title with the clean versions\n",
    "REPLACE = True\n",
    "if REPLACE:\n",
    "    data['title'] = data['title_clean']\n",
    "    data['content'] = data['content_clean']\n",
    "    data = data.drop(['title_clean', 'content_clean'], axis = 1)\n",
    "    \n",
    "# Step 5: Create quantitative features\n",
    "data = create_fea(data)\n",
    "\n",
    "# Step 6: Fix spelling - not yet implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.today().strftime('%Y_%m_%d')\n",
    "data.to_csv(f'clean_data/{today}_dfs_clean.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove largest 5% of reviews\n",
    "- Note this will skew the results, as negative ones tend to be longer, but otherwise our top few reviews may be 1000's of words, and be outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pct_to_drop = 0.02\n",
    "data = data.sort_values('content_num_words', ascending=False).iloc[round(data.shape[0] * top_pct_to_drop):,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove old reviews\n",
    "- Do we just want to investigate say the last year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Oldest review:\", data['date'].min(), \", Newest review:\", data['date'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REMOVE_OLD = '2020-01-01' # For example, set REMOVE_OLD to '2015-01-01'\n",
    "if REMOVE_OLD:\n",
    "    data = data[data['date'] > REMOVE_OLD]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the breakdown of 1 - 5 stars?\n",
    "- No we don't need a funnel and a pie chart, but why not "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart(df = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_star_funnel(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stratify reviews?\n",
    "- If we have a heavily skewed dataset, we may want to take a random sample of X from each of the buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample_df(df, col, n_samples):\n",
    "    n = min(n_samples, df[col].value_counts().min())\n",
    "    df_ = df.groupby(col).apply(lambda x: x.sample(n))\n",
    "    df_.index = df_.index.droplevel(0)\n",
    "    return df_\n",
    "\n",
    "STRATIFY_REVIEWS = False\n",
    "if STRATIFY_REVIEWS:\n",
    "    data = stratified_sample_df(data, 'num_stars', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_stars'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist3(data[data['num_stars'] == 5], 'content_num_char',\n",
    "       'Characters Per \"Positive review')\n",
    "plot_dist3(data[data['num_stars'] == 1], 'content_num_char',\n",
    "       'Characters Per \"Negative review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_word_len_histogram(data[data['num_stars'] == 5]['content'],\n",
    "                       data[data['num_stars'] == 1]['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dist3(data[data['num_stars'] == 5], 'content_num_words',\n",
    "       'Words Per \"Positive review')\n",
    "plot_dist3(data[data['num_stars'] == 1], 'content_num_words',\n",
    "       'Words Per \"Negative review')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Length of review by stars\n",
    "- Dosen't always look good if our dataset is heavily skewed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 2)\n",
    "g = sns.FacetGrid(data, col='num_stars', height=4)\n",
    "g.map(plt.hist,'content_num_char')\n",
    "plt.subplots_adjust(top=0.8)\n",
    "g.fig.suptitle(f'Size of review distribution, by number of stars')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# most common words\n",
    "- Content is the body of the review\n",
    "- Title is the title of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_trust import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams(df = data, n = 1, title = 'Most Common Unigrams', mx_df = 0.9, content_or_title = 'content')\n",
    "ngrams(df = data, n = 1, title = 'Most Common Unigrams', mx_df = 0.9, content_or_title = 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most common bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams(df = data, n = 2, title = 'Most Common Bigrams', mx_df = 0.9, content_or_title = 'content')\n",
    "ngrams(df = data, n = 2, title = 'Most Common Bigrams', mx_df = 0.9, content_or_title = 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# most common trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams(df = data, n = 3, title = 'Most Common Trigrams', mx_df = 0.9, content_or_title = 'content')\n",
    "ngrams(df = data, n = 3, title = 'Most Common Trigrams', mx_df = 0.9, content_or_title = 'title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determining Topics\n",
    "### These are potentially the most powerful outputs, if correctly tuned\n",
    "#### We can do this for title or content as well\n",
    "- we'll be using a method called Non-Negative Matrix Factorization (NMF) to see if we can get some defined topics out of our TF-IDF matrix, with this way TF-IDF will decrease impact of the high frequency words, so we might get more specific topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_trust import display_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Topics for TITLE of review')\n",
    "display_topics(data[data['num_stars'] == 5]['title'], \n",
    "               no_top_words = 5,\n",
    "               topic = 'Positive review topics \\n',\n",
    "               components = 2)\n",
    "print('\\n======================================\\n')\n",
    "print('\\n======================================\\n')\n",
    "print('Topics for BODY of review')\n",
    "display_topics(data[data['num_stars'] == 5]['content'], \n",
    "               no_top_words = 5,\n",
    "               topic = 'Positive review topics \\n',\n",
    "               components = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Topics for TITLE of review')\n",
    "display_topics(data[data['num_stars'] == 1]['title'], \n",
    "               no_top_words = 5,\n",
    "               topic = 'Negative review topics \\n',\n",
    "               components = 2)\n",
    "print('\\n======================================\\n')\n",
    "print('\\n======================================\\n')\n",
    "print('Topics for BODY of review')\n",
    "display_topics(data[data['num_stars'] == 1]['content'], \n",
    "               no_top_words = 5,\n",
    "               topic = 'Negative review topics \\n',\n",
    "               components = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding topics - Attempt 2 - Using `latent dirichlet allocation`\n",
    "- So the above gives us great informaton on topics, with context\n",
    "- Now we are just going to be looking for main topics, more similar to unigrams/bigrams/trigrams\n",
    "    - see https://miningthedetails.com/blog/python/lda/GensimLDA/ for tips on training\n",
    "\n",
    "- The printed topics, gives use insights into the actual components of the topics\n",
    "\n",
    "\n",
    "- The visualisation, allows us to evaluate the term frequency inside each of the topics, to see what they are comprised of, from the reviews perspecitve, not the modelling perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DO_LDA = True\n",
    "\n",
    "CONTENT_OR_TITLE = 'content'\n",
    "\n",
    "\n",
    "reviews_split = data[CONTENT_OR_TITLE].str.split()\n",
    "# creating the term dictionary of our corpus, where every unique term is assigned an index\n",
    "dictionary = corpora.Dictionary(reviews_split)\n",
    "# convert the list of reviews (reviews_2) into a Document Term Matrix \n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in reviews_split]\n",
    "\n",
    "# Creating the object for LDA model using gensim library\n",
    "LDA = gensim.models.LdaMulticore # .ldamodel.LdaModel\n",
    "\n",
    "# Build LDA model - we have picked 3 main topics\n",
    "# switch to sklearn.decomposition.LatentDirichletAllocation for consistency?\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n",
    "                chunksize=1000, passes=50, workers=3)\n",
    "# print out the topics that our LDA model has learned\n",
    "lda_model.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import inspect\n",
    "import IPython\n",
    "lines = inspect.getsource()\n",
    "print(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try hovering over the circles on the left, or the words on the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "pyLDAvis.enable_notebook()\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "v = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary, plot_opts = 'ylab')\n",
    "#pyLDAvis.save_html(v, 'lda.html')\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(filename='lda.html') # Try with json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the same as the above, but with the title of the review, not the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_LDA:\n",
    "    CONTENT_OR_TITLE = 'title'\n",
    "\n",
    "    reviews_split = data[CONTENT_OR_TITLE].str.split()\n",
    "    # creating the term dictionary of our corpus, where every unique term is assigned an index\n",
    "    dictionary = corpora.Dictionary(reviews_split)\n",
    "    # convert the list of reviews (reviews_2) into a Document Term Matrix \n",
    "    doc_term_matrix = [dictionary.doc2bow(rev) for rev in reviews_split]\n",
    "\n",
    "    # Creating the object for LDA model using gensim library\n",
    "    LDA = gensim.models.LdaMulticore\n",
    "\n",
    "    # Build LDA model - we have picked 3 main topics\n",
    "    lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n",
    "                    chunksize=1000, passes=50, workers=3)\n",
    "    # print out the topics that our LDA model has learned\n",
    "    lda_model.print_topics()\n",
    "\n",
    "    # Visualize the topics\n",
    "    pyLDAvis.enable_notebook()\n",
    "    vis = pyLDAvis.gensim.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "    vis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above, we can use this to filter reviews into larger buckets, which gives us the breakdown of the pareto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_trust import plot_wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(df = data[data['num_stars'] == 5]['content'],\n",
    "               max_words=200, max_font_size=100, \n",
    "               title = 'Most common words of POSITIVE reviews',\n",
    "               title_size=50, image = 'basket.png',\n",
    "              more_stopwords = {'store', 'order'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcloud(df = data[data['num_stars'] == 1]['content'],\n",
    "               max_words=200, max_font_size=100, \n",
    "               title = 'Most common words of NEGATIVE reviews',\n",
    "               title_size=50, image = 'basket.png',\n",
    "              more_stopwords = {'store', 'order', 'one', 'morrisons'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add year filter\n",
    "- Why are we getting spikes on certain days?\n",
    "    - We can investigate previous drops, and see the causes, then compare to current drops\n",
    "- Maybe scale count to 1-5? (or 0-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_slider(df = data, window = 30, add_count = False, add_var = True, add_kurt = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ratings by weekday etc...\n",
    "- turn this to nice plots and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotting_trust import trend_barplots\n",
    "trend_barplots(df = data,\n",
    "               mean_or_count = 'count',\n",
    "               plots = ['wday', 'day', 'week', 'month', 'year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's group our reviews into categories\n",
    "- Then pareto the main complaints by stars? Stack bar chart\n",
    "\n",
    "- So for each review, we want a primary, and maybe a secondary reason\n",
    "\n",
    "- Let's use the categories previously used,\n",
    "\n",
    "\n",
    "    - Quality - Material\n",
    "    - Quality - Durability\n",
    "    - Quality - Finishing\n",
    "    - Quality - Uncomfortable\n",
    "\n",
    "\n",
    "    - Delivery - Order Delayed\n",
    "    - Delivery - Wrong Address\n",
    "    - Delivery - Damage\n",
    "    - Delivery - Charges/Trips\n",
    "\n",
    "\n",
    "    - Customer Service - Unresponsive\n",
    "    - Customer Service - Not Helpful\n",
    "    - Customer Service - Guarantee\n",
    "    - Customer Service - Order Updates\n",
    "    - Customer Service - Price Change\n",
    "    - Customer Service - Refunds/Finance\n",
    "    \n",
    "    \n",
    "    - Availability - Out of Stock\n",
    "    - Availability - Outdated\n",
    "\n",
    "\n",
    "    - Other\n",
    "\n",
    "\n",
    "#### Current analysis is very basic, this needs more work\n",
    "- So the main categories are quality, delivery, customer service, availability and other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['num_stars'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['is_quality'] = ['quality' in i for i in data['content'].str.split()]\n",
    "data['is_service'] = ['service' in i for i in data['content'].str.split()]\n",
    "data['is_availability'] = ['availability' in i for i in data['content'].str.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = data.groupby(['is_quality'])['num_stars']\n",
    "print('===COUNT===')\n",
    "print(grp.count())\n",
    "print('===MEAN===')\n",
    "print(grp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = data.groupby(['is_service'])['num_stars']\n",
    "print('===COUNT===')\n",
    "print(grp.count())\n",
    "print('===MEAN===')\n",
    "print(grp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp = data.groupby(['is_availability'])['num_stars']\n",
    "print('===COUNT===')\n",
    "print(grp.count())\n",
    "print('===MEAN===')\n",
    "print(grp.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's do some predictive modelling\n",
    "- The aim of this is to find which words are the best predictors, for a given score, which will help inform what is wrong with a company\n",
    "- https://www.kaggle.com/nicapotato/guided-numeric-and-text-exploration-e-commerce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- So our X data will be content\n",
    "- Our y data will be num_stars\n",
    "    - Where 1 and 2 stars = negative, \n",
    "    - 3 will be dropped (for now)\n",
    "    - 4 and 5 will be positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep data\n",
    "data_for_reg = data[data['num_stars'] != 3].copy()\n",
    "data_for_reg.loc[:,'target'] = -9999\n",
    "data_for_reg.loc[data_for_reg['num_stars'] < 3, 'target'] = 0 # 0 negative\n",
    "data_for_reg.loc[data_for_reg['num_stars'] > 3, 'target'] = 1 # 1 positive\n",
    "\n",
    "X_full = data_for_reg['content']\n",
    "y_full = data_for_reg['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer()\n",
    "X = vect.fit_transform(X_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y_full\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=23, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Train Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_train), y_train)))\n",
    "print(\"Train Set ROC: {}\\n\".format(metrics.roc_auc_score(model.predict(X_train), y_train)))\n",
    "\n",
    "print(\"Validation Set Accuracy: {}\".format(metrics.accuracy_score(model.predict(X_valid), y_valid)))\n",
    "print(\"Validation Set ROC: {}\".format(metrics.roc_auc_score(model.predict(X_valid), y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics.classification_report(model.predict(X_valid), y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the confustion matrix\n",
    "- These matrices are for the validation data\n",
    "    - That is data that the model is seeing for the first time\n",
    "    - So we only give the model the review, not the rating\n",
    "        - Then predict the rating with the model and the below matrix gives us the split on true positives, true negatives, false positives and false negatives\n",
    "            - This speaks to how \"good\" or \"bad\" our model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\\\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "sns.set(font_scale=2.0)\n",
    "for norm, j in zip(['true', None], axes):\n",
    "    plot_confusion_matrix(model, X_valid, y_valid, normalize = norm, ax = j)\n",
    "axes[0].set_title(f'Normalised confusion matrix', fontsize = 24)\n",
    "axes[1].set_title(f'Raw confusion matrix', fontsize = 24)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word importance using ELI5\n",
    "- Stand for \"Explain Like I'm 5\"\n",
    "- Used to debug machine learning classifiers and explain their predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_names = [0, 1]\n",
    "eli5.show_weights(model, vec=vect, top=100,\n",
    "                  target_names=target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we can run through each row, and find out what the model is doing\n",
    "- These scores are the contribution of each word to the desired result\n",
    "    - That is different from above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iteration in range(5):\n",
    "    samp = random.randint(1,data.shape[0])\n",
    "    print(\"Real Label: {}, on {}\".format(data['rating'].iloc[samp], data['date'].iloc[samp]))\n",
    "    display(eli5.show_prediction(model,data[\"content\"].iloc[samp], vec=vect,\n",
    "                         target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's used a more powerful model, and compare the results\n",
    "- Let's start with LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's build / use a sentiment model, and see how this corresponds to the stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
